{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75b36159",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ§­ Personal Codex Agent â€” Endâ€‘toâ€‘End (Notebook)\n",
    "\n",
    "This notebook lets you run the entire project in one place:\n",
    "\n",
    "1. Install dependencies\n",
    "2. Configure either **OpenAI** _or_ **Azure OpenAI**\n",
    "3. Load your personal docs (CV + 2â€“3 files) from `data/`\n",
    "4. Build a **FAISS** index with embeddings\n",
    "5. Ask questions about yourself with **RAG**\n",
    "\n",
    "> **Tip:** Put your files inside a local `data/` folder next to this notebook before running.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d307ccf",
   "metadata": {},
   "source": [
    "## 1) Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "id": "eeb24c1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T13:49:31.563095Z",
     "start_time": "2025-09-02T13:49:30.956102Z"
    }
   },
   "source": [
    "\n",
    "# If running in a fresh environment, install requirements.\n",
    "# You can safely re-run this cell.\n",
    "import sys, subprocess\n",
    "\n",
    "def pip_install(pkgs):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *pkgs])\n",
    "\n",
    "pip_install([\n",
    "    \"openai>=1.40\",\n",
    "    \"python-dotenv>=1.0\",\n",
    "    \"faiss-cpu\",\n",
    "    \"pypdf\",\n",
    "    \"numpy>=1.26\",\n",
    "    \"tiktoken>=0.7\",\n",
    "])\n",
    "print(\"âœ… Installed / verified packages.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai>=1.40 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.93.3)\n",
      "Requirement already satisfied: python-dotenv>=1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.1.1)\n",
      "Requirement already satisfied: faiss-cpu in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.12.0)\n",
      "Requirement already satisfied: pypdf in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (6.0.0)\n",
      "Requirement already satisfied: numpy>=1.26 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.2.6)\n",
      "Requirement already satisfied: tiktoken>=0.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.11.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai>=1.40) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai>=1.40) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai>=1.40) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai>=1.40) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai>=1.40) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai>=1.40) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai>=1.40) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openai>=1.40) (4.13.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.40) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.40) (3.10)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=1.40) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=1.40) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.40) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai>=1.40) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai>=1.40) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai>=1.40) (0.4.1)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tiktoken>=0.7) (2025.8.29)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from tiktoken>=0.7) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken>=0.7) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken>=0.7) (2.4.0)\n",
      "âœ… Installed / verified packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.1.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.2\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "13ce1f16",
   "metadata": {},
   "source": [
    "## 2) Configure provider (OpenAI **or** Azure OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "id": "bdd8358e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T13:49:34.041781Z",
     "start_time": "2025-09-02T13:49:31.571938Z"
    }
   },
   "source": [
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "PROVIDER = \"openai\"  # change to \"azure\" if you want to use Azure OpenAI\n",
    "\n",
    "if PROVIDER == \"openai\":\n",
    "    print(\"ðŸ”‘ Using OpenAI (platform.openai.com)\")\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OPENAI_API_KEY: \")\n",
    "    # (Optional) choose models\n",
    "    CHAT_MODEL = os.getenv(\"MODEL\", \"gpt-4o-mini\")\n",
    "    EMBED_MODEL = os.getenv(\"EMBED_MODEL\", \"text-embedding-3-small\")\n",
    "else:\n",
    "    print(\"ðŸ”‘ Using Azure OpenAI\")\n",
    "    # You need an Azure OpenAI resource with deployments already created.\n",
    "    def prompt_if_missing(var, prompt):\n",
    "        if not os.getenv(var):\n",
    "            os.environ[var] = input(prompt).strip()\n",
    "        return os.getenv(var)\n",
    "\n",
    "    AZURE_OPENAI_ENDPOINT = prompt_if_missing(\"AZURE_OPENAI_ENDPOINT\", \"Azure endpoint (e.g. https://YOUR_RESOURCE.openai.azure.com): \")\n",
    "    AZURE_OPENAI_API_VERSION = prompt_if_missing(\"AZURE_OPENAI_API_VERSION\", \"API version (e.g. 2024-06-01): \")\n",
    "    AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\") or getpass(\"Paste your AZURE_OPENAI_API_KEY: \")\n",
    "    os.environ[\"AZURE_OPENAI_API_KEY\"] = AZURE_OPENAI_API_KEY\n",
    "    # Deployment names you created in Azure OpenAI Studio\n",
    "    AZURE_OPENAI_DEPLOYMENT = prompt_if_missing(\"AZURE_OPENAI_DEPLOYMENT\", \"Chat deployment name (e.g. gpt-4o-mini): \")\n",
    "    AZURE_OPENAI_EMBED_DEPLOYMENT = prompt_if_missing(\"AZURE_OPENAI_EMBED_DEPLOYMENT\", \"Embedding deployment name (e.g. text-embedding-3-small): \")\n",
    "    CHAT_MODEL = AZURE_OPENAI_DEPLOYMENT\n",
    "    EMBED_MODEL = AZURE_OPENAI_EMBED_DEPLOYMENT\n",
    "\n",
    "print(\"âœ… Config complete.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”‘ Using OpenAI (platform.openai.com)\n",
      "âœ… Config complete.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "a8bb213f",
   "metadata": {},
   "source": [
    "## 3) Helpers to load docs and chunk text"
   ]
  },
  {
   "cell_type": "code",
   "id": "830d78d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T13:49:34.066673Z",
     "start_time": "2025-09-02T13:49:34.057451Z"
    }
   },
   "source": [
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from pypdf import PdfReader\n",
    "\n",
    "def load_texts(dirpath: str):\n",
    "    \"\"\"Yield (doc_id, text) for .txt, .md, .pdf files.\"\"\"\n",
    "    p = Path(dirpath)\n",
    "    for fp in sorted(p.glob(\"*\")):\n",
    "        if fp.suffix.lower() in [\".txt\", \".md\"]:\n",
    "            text = fp.read_text(errors=\"ignore\")\n",
    "            yield fp.name, text\n",
    "        elif fp.suffix.lower() == \".pdf\":\n",
    "            try:\n",
    "                reader = PdfReader(str(fp))\n",
    "                pages = [page.extract_text() or \"\" for page in reader.pages]\n",
    "                yield fp.name, \"\\n\".join(pages)\n",
    "            except Exception as e:\n",
    "                print(f\"[warn] Could not read {fp.name}: {e}\")\n",
    "\n",
    "def chunk(text: str, max_tokens: int = 400):\n",
    "    \"\"\"Very rough chunking by double-newlines.\"\"\"\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    parts = text.split(\"\\n\\n\")\n",
    "    buf, count = [], 0\n",
    "    for part in parts:\n",
    "        tokens = part.split()\n",
    "        if count + len(tokens) > max_tokens and buf:\n",
    "            yield \" \".join(buf)\n",
    "            buf, count = [], 0\n",
    "        buf.append(part)\n",
    "        count += len(tokens)\n",
    "    if buf:\n",
    "        yield \" \".join(buf)\n",
    "\n",
    "print(\"âœ… Utils ready.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Utils ready.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "54e23420",
   "metadata": {},
   "source": [
    "## 4) Build the FAISS index from `data/`"
   ]
  },
  {
   "cell_type": "code",
   "id": "d4117695",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T13:49:37.345605Z",
     "start_time": "2025-09-02T13:49:34.106947Z"
    }
   },
   "source": [
    "\n",
    "import numpy as np\n",
    "import faiss, pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Choose client based on provider\n",
    "if PROVIDER == \"openai\":\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "else:\n",
    "    from openai import AzureOpenAI\n",
    "    client = AzureOpenAI(\n",
    "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "        api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    )\n",
    "\n",
    "records = []\n",
    "for doc_id, text in load_texts(\"data\"):\n",
    "    for c in chunk(text, max_tokens=400):\n",
    "        c = c.strip()\n",
    "        if c:\n",
    "            records.append({\"doc_id\": doc_id, \"text\": c})\n",
    "\n",
    "print(f\"Loaded {len(records)} chunks. Embeddingâ€¦\")\n",
    "\n",
    "vectors = []\n",
    "BATCH = 64\n",
    "for i in tqdm(range(0, len(records), BATCH)):\n",
    "    batch = [r[\"text\"] for r in records[i:i+BATCH]]\n",
    "    resp = client.embeddings.create(model=EMBED_MODEL, input=batch)\n",
    "    for d in resp.data:\n",
    "        vectors.append(d.embedding)\n",
    "\n",
    "xb = np.array(vectors, dtype=\"float32\")\n",
    "index = faiss.IndexFlatL2(xb.shape[1])\n",
    "index.add(xb)\n",
    "\n",
    "# Optionally save to disk\n",
    "Path(\"index\").mkdir(exist_ok=True)\n",
    "faiss.write_index(index, \"index/faiss.index\")\n",
    "with open(\"index/docs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(records, f)\n",
    "\n",
    "print(\"âœ… Index built and saved to ./index\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 30 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 chunks. Embeddingâ€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRateLimitError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 30\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(records), BATCH)):\n\u001B[1;32m     29\u001B[0m     batch \u001B[38;5;241m=\u001B[39m [r[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m r \u001B[38;5;129;01min\u001B[39;00m records[i:i\u001B[38;5;241m+\u001B[39mBATCH]]\n\u001B[0;32m---> 30\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membeddings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mEMBED_MODEL\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m resp\u001B[38;5;241m.\u001B[39mdata:\n\u001B[1;32m     32\u001B[0m         vectors\u001B[38;5;241m.\u001B[39mappend(d\u001B[38;5;241m.\u001B[39membedding)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/openai/resources/embeddings.py:129\u001B[0m, in \u001B[0;36mEmbeddings.create\u001B[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[1;32m    123\u001B[0m             embedding\u001B[38;5;241m.\u001B[39membedding \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mfrombuffer(  \u001B[38;5;66;03m# type: ignore[no-untyped-call]\u001B[39;00m\n\u001B[1;32m    124\u001B[0m                 base64\u001B[38;5;241m.\u001B[39mb64decode(data), dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfloat32\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    125\u001B[0m             )\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[1;32m    127\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\n\u001B[0;32m--> 129\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    130\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/embeddings\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    131\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEmbeddingCreateParams\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    132\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    133\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    134\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    135\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    136\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    137\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpost_parser\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    138\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    139\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mCreateEmbeddingResponse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    140\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/openai/_base_client.py:1249\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1235\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mpost\u001B[39m(\n\u001B[1;32m   1236\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1237\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1244\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1245\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m   1246\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[1;32m   1247\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[1;32m   1248\u001B[0m     )\n\u001B[0;32m-> 1249\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/openai/_base_client.py:1037\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[0;34m(self, cast_to, options, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1034\u001B[0m             err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[1;32m   1036\u001B[0m         log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRe-raising status error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1037\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m   1041\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcould not resolve response (should never happen)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[0;31mRateLimitError\u001B[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "5f628f2f",
   "metadata": {},
   "source": [
    "## 5) Ask questions with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7104a4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embed_query(q: str):\n",
    "    e = client.embeddings.create(model=EMBED_MODEL, input=[q]).data[0].embedding\n",
    "    return np.array(e, dtype=\"float32\").reshape(1, -1)\n",
    "\n",
    "def retrieve(query: str, k=4):\n",
    "    qv = embed_query(query)\n",
    "    D, I = index.search(qv, k)\n",
    "    return [records[i][\"text\"] for i in I[0]]\n",
    "\n",
    "def ask_llm(query: str, contexts):\n",
    "    system = (\n",
    "        \"You are the user's personal codex. \"\n",
    "        \"Answer using the retrieved context when relevant; be specific and truthful.\"\n",
    "    )\n",
    "    context_block = \"\\n\\n\".join([f\"[Context {i+1}]\\n{c}\" for i, c in enumerate(contexts)])\n",
    "    messages = [\n",
    "        {\"role\":\"system\",\"content\":system},\n",
    "        {\"role\":\"user\",\"content\": f\"Context:\\n{context_block}\\n\\nQuestion: {query}\"}\n",
    "    ]\n",
    "    resp = client.chat.completions.create(model=CHAT_MODEL, messages=messages)\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "print(\"âœ… RAG helpers ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90a0862",
   "metadata": {},
   "source": [
    "### Try it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c469eb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "question = \"What kind of engineer am I, and what projects am I proud of?\"\n",
    "ctx = retrieve(question, k=4)\n",
    "answer = ask_llm(question, ctx)\n",
    "print(\"---- ANSWER ----\\n\", answer)\n",
    "print(\"\\n---- CONTEXT SNIPPETS ----\")\n",
    "for i, c in enumerate(ctx, 1):\n",
    "    print(f\"\\n[Chunk {i}]\\n{c[:700]}...\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
