# Commit Log — AI vs Manual Contributions

This log documents which parts of the project were AI-assisted and which were written/edited manually.

---

## Initial Setup
- Manual: Created virtual environment, installed dependencies (`requirements.txt`).
- Manual: Added `.env` with API keys.

---

## Core Code Files

### rag.py
- AI: Drafted initial `load_texts` and `chunk` functions.
- Manual: Rewrote chunking with overlap strategy, tested on real docs, ensured robustness for long paragraphs.

### prompt.py
- AI: Suggested base `SYSTEM_PROMPT` structure.
- Manual: Refined wording into my authentic tone, ensuring answers stayed grounded and reflective of me.

### build_index.py
- AI: Scaffolded embedding loop, FAISS index creation, and persistence of metadata.
- Manual: Added error handling for empty data folder, validated batch embeddings, ensured reproducibility.

### app.py
- AI: Provided Streamlit scaffolding, retrieval helpers, and chat flow.
- Manual: Customized UI labels, improved error messaging, added context expander, refined answer style to keep “my voice.”

---

## Artifacts
- AI: Helped outline artifact structure (`prompts_used.md`, `agent_instructions.md`, `commit_log.txt`).
- Manual: Populated with real examples, annotated AI prompts vs. my edits, added reflections on workflow.

---

## Deployment Prep
- Manual: GitHub setup, secret handling (.env ignored).
- Manual: Planning Streamlit Cloud deployment.

---

## Summary
- Roughly **60% AI-assisted scaffolding**, **40% manual edits, curation, and refinement**.
- AI accelerated the boilerplate and structure.
- My manual input shaped the dataset, ensured correctness, polished the voice, and added reflection — making the agent feel personal and representative of me.
